.LUJ
AD
UCC
AU
1
6
lb]4.lU4J/Vl
LL.
WA
1
V
(
Abstract-A
central
machine
is
interested
in
estimating
the
underlying
structure
of
a
sparse
Gaussian
Graphical
Model
(GGM)
from
datasets
distributed
across
multiple
local
machines.
The
local
machines
can
communicate
with
the
central
machine
through
a
wireless
multiple
access
channel.
In
this
paper,
we
are
interested
in
designing
effective
strategies
where
reliable
learning
is
feasible
under
power
and
bandwidth
limitations.
Two
approaches
are
proposed:
Signs
and
Uncoded
methods.
In
Signs
method,
the
local
machines
quantize
their
data
into
binary
vectors
and
an
optimal
channel
coding
scheme
is
used
to
reliably
send
the
vectors
to
the
central
machine
where
the
structure
is
learned
from
the
received
data.
In
Uncoded
method,
data
symbols
are
scaled
and
transmitted
through
the
channel.
The
central
machine
uses
the
received
noisy
symbols
to
recover
the
structure.
Theoretical
results
show
that
both
methods
can
recover
the
structure
with
high
probability
for
large
enough
sample
size.
Experimental
results
indicate
the
superiority
of
Signs
method
over
Uncoded
method
under
several
circumstances.
Index
Terms-Structure
learning,
Gaussian
graphical
model,
distributed
learning
I.
INTRODUCTION
N
recent
years,
by
the
explosion
of
the
volume
of
training
data,
distributed
machine
learning
has
become
more
impor-
tant
than
ever.
Many
modern
big
datasets
are
distributed
over
several
hosting
machines
which
are
connected
to
each
other
via
some
communication
links,
In
such
systems,
designing
distributed
learning
algorithms
which
efﬁciently
exploit
the
available
resource
demands
a
careful
design
where
intensive
computational
workloads
and
the
amount
of
data
communica-
tions
are
taken
into
account.
This
paper
is
focused
on
the
problem
of
structure
learn-
ing
in
Gaussian
Graphical
Models
(GGMs)
in
distributed
environments.
A
GGM
for
a
d-dimensional
random
vector
x
:
(x1,
-
--
,x,,)T
6
Rd
is
speciﬁed
by
a
graph
Q(’V,8)
where
W
:
{l,---
,d}
is
the
set
of
vertices
and
8
g
'V2
is
the
set
of
edges.
The
model
comprises
all
d-dimensional
normal
distributions
N01,
6")
where
G)
is
the
precision
matrix
with
@fk
at
0
iff
(j,k)
E
8.
It
worths
mentioning
that
GGM
is
indeed
a
Markov
Random
Field
(MRF).
In
our
problem
setting,
we
assume
that
the
data
are
dis-
tributed
over
multiple
local
machines
so
that
each
one
contains
a
single
dimension
of
the
whole
dataset.
The
local
machines
are
connected
to
a
central
machine
via
a
wireless
medium.
The
central
machine
is
responsible
for
inferring
the
conditional
dependencies
between
the
gathered
data
by
the
local
machines.
The
system’s
block
diagram
is
depicted
in
Figure
1.
We
also
assume
that
the
communication
links
between
the
local
and
the
central
machine
are
bandwidth
limited
implying
that
transmission
of
the
whole
local
datasets
to
the
central
machine
is
impossible.
Due
to
this
constraint,
each
local
machine
transmits
some
information
from
its
local
dataset
to
the
central
machine.
Then,
the
central
machine
estimates
the
underlying
graph
structure
using
received
information
from
the
local
machines.
In
this
paper,
we
consider
a
wireless
multiple
access
channel
between
the
local
machines
and
the
central
node.
Each
local
machine
is
equipped
with
a
single
antenna
while
the
central
node
is
equipped
with
multiple
antennas.
Hence,
the
overall
channel
between
the
local
and
central
machines
is
modeled
as
a
single-input
multiple»output
multiple
access
channel
(SIMO-
MAC).
We
have
proposed
two
communication
schemes
for
trans-
mitting
information
from
the
local
machines
to
the
central
machine.
In
the
ﬁrst
scheme,
we
have
separate
the
source
coding
from
the
channel
coding.
In
this
scheme,
we
quantize
the
source
samples
into
single
bits
and
assume
that
there
exists
a
channel
coding
such
that
the
bits
can
be
sent
through
the
channel
reliably.
We
refer
to
this
scheme
as
Signs
method
in
the
paper.
In
the
second
scheme,
we
do
not
use
any
source
and
channel
coding.
In
this
scheme,
we
put
the
source
samples
into
the
channel
without
any
encoding.
At
the
central
machine,
we
directly
estimate
the
underlying
graph
structure
using
received
data
from
the
channel.
We
refer
to
this
scheme
as
Uncoded
method,
We
have
shown
through
theoretical
analysis
and
experi-
ments
that
by
transmitting
only
I
bit
per
sample;
the
central
machine
can
reliably
recover
the
underlying
graph
structure.
More
precisely,
we
have
shown
theoretically
that
by
consum-
ing
only
I
bit
per
sample,
under
some
mild
conditions,
the
central
machine
can
perfectly
recover
the
graph
structure
with
high
probability.
Moreover,
the
true
signs
of
the
edges
weights
of
the
graph
are
obtained.
The
paper
is
organized
as
follows.
In
Section
II,
we
provide
a
brief
review
on
structure
learning
of
GGMs.
Section
III
describes
the
detail
of
our
modeling
for
the
source
and
the
communication
channel.
In
sections
IV
and
V.
we
describe
Signs
and
Uncoded
methods,
respectively.
Section
VI
provides
experimental
results
to
compare
and
evaluate
the
proposed
methods.
Finally.
Section
VII
concludes
the
paper.
II.
RELATED
WORK
The
problem
of
structure
learning
of
GGMS
from
data
samples
has
applications
in
many
ﬁelds
including
biology
loslafa
Tavassolipour,
Armin
Karamzade,
Reza
Mirzaeifard,
Seyed
Abolfazl
Molahari,
and
Mohammad-Taghi
Manzuri
Shalmani
Access
N<
\Ietworks
a1
networks.
For
example,
it
has
been
used
for
gene
y
networks
reconstruction
in
[l],
[2]
and
analysis
of
ationships
in
social
networks
in
[3].
There
are
many
ddressing
this
problem
from
various
perspectives
[4],
how-Lin
algorithm
obtains
the
maximum
likelihood
of
the
structure
if
the
underlying
graph
is
a
tree
ough
this
algorithm
is
applicable
for
discrete
random
,
it
can
be
used
for
tree
structured
GGMs
in
a
similar
[8].
Tavassolipour
et
a1,
[9]
proposed
a
distributed
)f
the
Chow-Liu
algorithm
and
proved
it
can
recover
rlying
tree
structure
with
high
probability.
Tan
et
a1.
nd
[1
1]
provided
an
analysis
of
the
error
exponent
of
v-Liu
algorithm
on
tree-structured
GGMs.
l
have
the
property
that
the
neighbors
of
each
variable
)tained
by
solving
a
linear
regression
problem
for
the
riding
variable
on
other
variables.
This
approach
is
to
as
neighborhood
selection
in
the
literature
For
the
ructures,
there
are
some
methods
which
penalize
the
gression
problem
with
[I
of
the
coefﬁcient
vector
[12],
g
the
proposed
methods
for
sparse
structure
esti-
)f
GGMs,
the
[1
-regularized
maximum
likelihood
res
are
more
popular
[6],
[12],
[14],
This
class
of
analyzed
in
several
articles
(see
[4]
and
refs
therein),
nee,
Ravikumar
et
al.
[
4]
analyzed
the
performance
of
gularized
maximum
likelihood
estimator
(MLE)
under
tensional
scaling.
They
showed
that
with
probability
1g
to
one,
the
estimated
structure
correctly
speciﬁes
pattern
of
the
true
precision
matrix.
A
similar
study
cted
by
[15]
which
analyzed
the
consistency
of
the
rized
MLE
in
the
Frobenius
norm,
ts
the
lasso
typed
estimators,
thresholding
based
ese
are
proposed
for
sparse
recovering
of
the
precision
3],
[8].
For
example,
Sojoudi
[5]
proposed
a
simple
ling
method
and
showed,
under
certain
conditions,
the
structure
is
identical
to
the
structure
obtained
by
the
differs
from
are
split
acr
samples.
Me
matrix
in
a
precision
m:
We
are
E
dimensional
(Qx)jj
=
estimating
t
ex
=
Q;'
separate
loc;
dimension
c
Denoting
machine
car
We
denote
I
Xm
=
mi),
machine
is
The
prob:
is
given
by
f
The
negativr
where
SX
:
ance
matrix,
sparse
preci:
likelihood
fl
where
HGH
The
whole
syslem
block
diagram.
distributed
setting,
there
are
several
studies
which
regularizatic
he
problem
of
covariance/precision
matrix
estimation
35
glasso
[1‘
],
[l8].
Arroyo
and
Hon
[18]
studied
the
problem
of
In
[4],
it
i
'ecision
matrix
estimation
in
the
situation
where
the
are
distributed
among
several
machines,
Their
work
0m
our
setting
in
the
sense
that
we
assume
the
data
across
dimensions
whereas
they
split
the
data
across
Meng
et
al.
[
16]
addressed
estimation
of
the
precision
i
a
distributed
manner
where
the
zero
pattern
of
the
matrix
is
known
in
advanced.
e
given
n
i.i,d.
random
vectors
drawn
from
a
d-
Inal
Zero
mean
normal
distribution
N(0,QX)
with
=
l,
The
focus
of
this
paper
is
the
problem
of
g
the
zero
pattern
of
the
sparse
precision
matrix
'I
in
a
situation
where
the
data
is
stored
in
of
local
machines
such
that
each
machine
possesses
one
In
of
the
sample
vectors.
mg
the
whole
gathered
data
by
(xm,
«
.1
,
xm},
the
jth
captures
the
j
th
dimension
of
the
sample
vectors.
te
the
j
th
dimension
of
the
1'7
th
sample
by
x(
),
i.e.
1'),
.xL'JIT
.
Hence,
the
local
data
at
the
j-
-th
local
I)
.
,
th
is
{x
obability
density
function
of
the
normal
distribution
by
:tive
log-likelihood
of
n
i.i.d.
samples
is
given
by
.
.
T
:
%
;.’:l
X“)
(xm)
is
reffered
to
as
sample
covari-
rix.
One
of
the
well
known
methods
for
estimating
the
ecision
matrix
is
to
solve
an
[l-regularized
maximum
d
function
stated
as
6H1,01‘f
I:
21*“
IG/ki,
and
A”
is
a
user
deﬁned
ation
parameter.
There
is
an
efﬁcient
algorithm
known
r
[19]
for
solving
the
above
log-determinant
program.
it
is
stated
that
the
Hessian
matrix
of
(2)
is
given
by
:
;
x/del(2n®’1)exp{--xr®x}l
(I)
gig
{g(®)
+
A”
HOHM}.
(3)
(mm
=
9:
®®;'-
<4)
‘BLEM
FORMULATION
(9507103:
det(@),
(2)
nel
Model
lat
V
:
{Lu}
,d}t
Note
that
S(€)X)
includes
the
entries.
Let
SCH-3X)
be
the
complement
set
of
S(®X)
cludes
all
pairs
0,
k)
where
{(55),},
:
0.
For
any
two
‘
and
T’
of
VXV,
the
notation
FTTr
denotes
a
lT|><|T’|
ix
of
F
with
rows
and
columns
indexed
by
T
and
T’,
ely.
opt
the
incoherence
condition
used
by
Ravikumar
el
‘
obtain
error
bounds
on
consistency
of
the
solutions.
le
max-row-sum
norm
of
a
d
by
d
matrix
X
as
We
assun
on
its
local
1
samples
{1}.
dataset
(flu
the
binary
(
per
sample
Denoting
R,,
the
achi
lion
1.
(
Incoherence
Condition
[4])
£er
xome
a
E
(0,
l]
.mch
that
|S|
X
\S‘
ide
base
2.
Thr
all
i
‘plication
of
the
above
condition
is
that
the
non-edge
mot
have
strong
inﬂuence
on
the
edges‘
tion
2.
(
Covariance
Control
[4])
ist
constants
K):,
l<1'
<
so
such
that
Jove
assumptions
imply
that
the
covariance
elements
y
row
of
Q‘
and
Pg;
have
bounded
[I
norms.
g
no
access
to
the
original
data,
the
central
machine
2
solution
of
the
optimization
problem
(3)
using
data
from
the
local
machines.
The
likelihood
function
for
the
sam;
estimate
the
using
the
re
estimator
fc
show
its
err
,ll
transmitters
have
equal
transmit
power
which
is
by
pi
This
implies,
for
all
j,
the
following
constraint
ansmit
symbols
should
be
satisﬁed:
The
equatio
is
the
Kronecker
matrix
product.
The
entry
FUMUJH)
where
H
s
nds
to
the
second
derivative
%,
evaluated
at
plex
matrix
jlx
I
m
and
FUZk),(Lm)
=
colejkaumL
are
draWn
ﬁne
the
set
of
non-zero
entries
in
the
true
precision
Gaussian
d‘
t
aﬁ
circularly
s)
X
I
-2
,
2)
depends
on
the
samples
via
the
sample
covariance
L61
Xj
8‘
X,
Thus,
obtaining
an
appropriate
approximate
of
the
variances
‘dI
ovariance
matrix
at
the
central
machine
would
result
COWCSPOHdiW
:l
estimate
of
the
underlying
structure.
function
(PI
'al
node.
In
this
paper,
we
assume
that
the
channel
nodeled
as
a
singlerinput
multipleioutput
Gaussian
access
channel
(SIM07GMAC)
with
additive
white
1e
number
of
antennas
at
the
receiver
is
assumed
to
where
ﬂy
6
i),
_
_
.
is
optimal
i
.
s
are
the-channel
Inputs
at
the
local
machlne
].
variance
(U
;
the
lransmll
symbols
by
vector
5,
the
channel
output
ed
by
y
:
Hs
+
z,
(1
I)
a
wireless
channel
between
the
local
machines
and
{<12
k)
e
Wem
¢
0).
(5)
(7514.
11'1“le
”3
.
§S)7|“|oo
S
(1
-
a).
(
7)
columns
in(
d
[3334
I;
‘Xijlr
b9
[20]
Sims
(10)
Thus,
by
p1
obtain
an
e‘
(9)
cmnes.
in
01
obtained
by
>
K2,
(8)
problem
(3:
rhinpc
In
N
trix,
In
the
fading
environments,
the
channel
gains
In
from
independent
circularly
symmetric
cnmplex
l
distribution.
The
additive
noise
2
is
an
independent
/
symmetric
Gaussian
vector
with
covaraience
matrix
sume
that
each
local
machine
applies
a
sign
function
:al
dataset
to
obtain
binary
data.
More
precisely,
given
(X10),
-
.
,
,
xii”)
at
local
machine
j,
it
obtains
the
signs
2;“,
-
-
~
,
301))
where
)3,
=
sign(x,),
Then,
it
transmits
'y
data
to
the
central
machine
with
the
rate
of
1
bit
)le
using
a
channel
encoder
and
decoder:
ing
the
bit
rate
of
the
channel
at
local
machine
j
by
chievable
bit
rates
for
all
machines
are
characterized
s
lgdet(%H§I-l5+ltsl).
v
s
g
{1,m
,d),
z
(12)
5
is
the
sub-matrix
of
H
that
includes
the
rows
and
indexed
by
S,
HH
is
the
Hermitian
of
H,
[‘5'
is
the
identity
matrix,
and
lg(-)
is
the
logarithm
function
in
[‘hroughout
this
section,
we
assume
that
Ri
2
1,
for
,-
-
-
,(1.
Therefore,
there
exists
a
channel
encoding
tn
1
bit
per
sample
at
each
local
machine.
central
machine,
our
goal
is
to
solve
the
optimization
(3)
on
the
received
binary
data
from
all
local
ma,
1
order
to
obtain
a
solution
that
is
close
to
the
solution
by
the
original
data,
we
seek
a
suitable
approximation
imple
covariance
matrix
Sr
in
(3)
Thus,
our
goal
is
to
the
sample
covariance
matrix
as
accurate
as
possible
2
received
signs
data.
In
this
section,
we
propose
an
'
for
the
sample
covariance
matrix
and
theoretically
error
decreases
exponentially
by
the
sample
size
ni
,-
and
xk
be
jointly
normal
with
zero
means,
unit
;
and
correlation
coefﬁcient
pjk.
If
2,
and
ﬁk
be
the
nding
sign
variables,
then
the
joint
probability
mass
(pmf)
of
f,
and
kk
is
given
by
[21]
Ation
(14)
can
be
rewritten
as
'
proposing
an
estimator
for
ﬁjk,
using
(15)
we
can
1
estimator
for
pjk.
The
following
estimator
for
181k
1]
in
the
sense
that
it
is
unbiased
and
has
minimum
(UMVE)
[22],
I
e
med
is
assumed
to
be
an
invertible
com-
,k
e
[0,
l]
and
given
by
l
MUM
,
5))
=
’C05(”ﬂjk)~
(15)
,1
+1
ﬁjk/Z
(1
’ﬁjk)/2
(13)
(l’ﬁjkvz
[ﬁx/2
(16)
arcsin(p,k)
II
(14)
vs
METHOD
.)
is
the
indicator
function,
ﬁjk
is
indeed
a
binomial
theorems
w
Iariable
with
success
probability
of
[if/C,
By
substitut-
GGM.
We
n
(15),
we
use
graph
strum
imator
for
pjki
Although
ﬂy
is
indeed
biased,
it
is
a
It
estimator
for
pjk.
Following
lemma
gives
an
error
1
this
estimator,
Theorem
1
mumucm:
1.
Let
xi
and
xk
be
two
jointly
normal
Variooles
Let
é):
be
th
7
means,
unit
Variances
and
correlation
coeﬁctent
xample
cow
n,
or
the
estimator
l7
,
we
have
\
f
<
)
A"
:
(Sn/a
ince
the
function
cos(«)
is
a
l-Lipschitz
function,
iie,
-
cos(y)|
s
Ix
-y|,
we
have
where
,-k
:
Ejkr
and
cos(.)
function
is
applied
on
the
input
ement
wise.
Le.
(
31),",-
:
-cos(72-rﬁ,»j).
By
substituting
a
sample
covariance
matrix
into
(3),
we
can
solve
the
ed
maximum
likelihood
problem.
hat
the
sample
covariance
matrix
:5:
deﬁned
in
(19)
cessarily
positive
semi-deﬁnite.
But,
it
does
not
affect
:Xity
and
uniqueness
solution
of
(3).
Ravikumar
et
al.
ed
that
the
problem
(3)
is
convex
and
has
a
unique
for
any
sample
covariance
matrix
with
strictly
positive
elements
which
holds
for
3;
in
(19).
‘orporating
Lemma
1
and
the
theorems
l
and
2
in
[4],
onclude
that
the
precision
matrix
obtained
by
solving
the
sample
covariance
matrix
:9;
in
(28).
recovers
the
:ture
with
high
probability.
Moreover.
the
proposed
correctly
recovers
the
signs
of
the
edges
with
high
ty.
precisely.
the
event
M(®X;éx)
indicates
that
@X
and
gree
on
the
zero
entries
and
for
the
nonzero
entries
;
the
same
sign.
Theorems
1
and
2
state
that
the
event
)X)
occurs
with
high
probability.
Before
stating
the
coding,
The
structure
usi
machine,
nc
structure
dir
As
descril
Each
local
I
each
channe
symbol
by
consequent
symbol.
Tht
received
tha
In
this
w:
r
where
j2
=
form
as
where
HR.)
imaginary
1:
;
is
sum
ofn
independent
Bernoulli
random
variables,
where
the
Hoeffding
inequalily
yields
Impleles
the
proof.
a
I
shows
that
the
error
of
proposed
estimator
ﬁjk
Remark
1.
lled
by
the
number
of
samples
exponentially,
Using
for
any
cha.
nator,
we
can
obtain
an
estimator
for
the
sample
:e
matrix
as
follows
-p,k\
2
6)
:
Pr(|
-
005(7r3jk)+cos(7r,8jk)\
2
(5)
then
”.
_
>
bye
SPT(”|B;I<
mas)
(,7)
1er
=
Pr(|/§/k
*ﬁjkl
2
g)
6
2
_,3jk\
2
;)
S
Zexp
(--n62),
”n
\V"!
«-,
2
’Pikl
2
6)
S
Zexp
(772mg),
(18)
(a)
[fthe
5
7r
edges
weigr
:osmBJ-k),
(17)
is
N53)»
(19)
In
‘his
St
2
its
local
dal
1.
Nate
Ihat
the
proposed
.rign
met/10:1
is
applicable
:harmel
wit/1
capacily
greater
than
or
equal
In
I
bit‘
l
we
should
deﬁne
some
properties
of
the
underlying
Ve
denote
the
maximum
degree
of
the
underlying
ructure
by
A.
The
minimum
absolute
value
of
the
:ighs
in
the
precision
matrix
is
denoted
by
6min
which
1
1.
Consider
a
normal
distribution
sati.
ying
the
nee
Assumption
1
and
2
with
parameter
a
E
(0.1].
e
the
solution
ofrhe
lagizleterminant
program
(3)
with
wvariance
SX
in
(19)
and
regularization
parameter
/a)‘l7i_1ln-fnrsume
0
<
5
<
(I
2.
Then
:6
sample
size
is
lower
bounded
as
s
section,
we
assume
that
each
local
machine
puts
data
into
the
channel
without
any
source
or
channel
The
central
machine
estimates
the
underlying
graph
using
received
data
from
the
channel,
At
the
central
no
source
or
channel
decoding
is
used
It
infers
the
directly
from
the
output
samples
of
the
channel.
;
cribed
in
Section
Ill-A,
we
consider
a
SIMO»GMAC,
a1
machine
can
transmit
two
consequent
samples
by
nnel
use.
More
precisely,
denoting
the
channel
input
by
s
=
SR
+
js,.
each
local
machine
can
put
two
ant
samples
as
real
and
imaginary
parts
of
the
input
Therefore,
at
the
central
machine,
n/Z
vectors
are
that
each
one
is
Zd-dimensional.
=
71‘
Hence,
it
can
be
rewritten
in
a
block-matrix
HR
’HI
SR
HI
HR
sI
YR
YI
ZR
1,
,
<25)
Rf],
are
d
X
[1
real
matrices,
and
all
:he
real
and
y
part
vectors
are
zl-dimensional.
In
this
way,
two
with
probability
at
least
l-dzé,
the
edge
set
speciﬁed
3‘
is
a
subset
of
the
true
edge
sets
,7
sample
size
satisﬁes
the
lower
bound
ay,
the
equation
(I
I)
can
be
decomposed
as
3
Z
Csig“
=
3V5”
max{Kz/<r,
KZKF},
Pr(M(@X;@\.))
2
17
ale.
(
23)
27r
max{k1-9
3A
mam/(21¢,
K%K%)),
mm’
+fHI)(SR
+jSI)+(ZR
+121).
(
24)
£320»
\
(99,-,-
L
(20)
JDED
METHOD
A
[Q
(
8
1+7
(Y
2
1n
7,
e
(21)
(
8
1+7
(Y
2
1n
7,
e
(22)
imples
can
be
transmitted
per
channel
use:
a
sample
0
the
real
part
and
the
other
is
put
into
the
imaginary
)articular,
SR
+js,
=
g(xk
+jxl),
where
xR
and
x,
ndependent
samples
mm
the
source
In
this
way,
the
power
constraints
are
satisﬁed
:ntral
machine
estimates
the
conditional
dependencies
tctors
x
using
the
received
vectors
yi
By
ﬁnding
‘
upper
boum
max
(Q
I;
and
Q5.
is
the
covariance
matrix
of
i
and
y,
ely,
By
substituting
the
sample
covariance
matrix
of
e
above
expression,
we
obtain
an
approximation
for
vle
covariance
matrix
of
i,
as
Lemma
3.
where
0,1
is
Je
deﬁne
the
random
variable
w
as
follows
‘ther
hand,
according
to
equation
(25),
we
have
Nex
2.
Given
n
i.i.d.
samples
(if
the
Vector
y.
Then,
for
rle
covariance
matrix
S);
in
(28)
we
have
Proof
Fran
Thus,
we
h:
7
2
~
2
c
:
3200
(1
+
ag/Amm(H))
,
(30)
Pr
(Kim;
(ﬂ)
is
the
minimum
eigenvalue
of
g
r
that
w
~
N(0,QW),
where
QM,
:
QX
+
(rzzﬁ'IITI'T.
;
w
:
[wl
,w,,]T,
Wj/1/(Qw)jj
is
a
standard
5
Sex}:
ariable
which
is
sub-Gaussian
with
parameter
l.
Thus,
g
to
Lemma
1
in
[4],
we
have
where
the
1
x
”My”
n
1:.
v
=
ﬁ'lsyﬁ‘T
is
the
sample
covariance
over
w“)
=
op‘imizatiou
;amples.
On
the
other
hand,
we
have
By
SUbSti
A
regularlzea
(17-1
(5;:
_
Q?)
ﬁ-T)
Z
5)
solution
for
-Qi)/k\2zs)=1>r(
1*
we
can
gum
=
Pr
(|
(5w
7
me
‘
Z
6)
the
underlyl
2
Theorem
2
n6
‘
S
exp
_-2
(3
)
mcoﬂerence
3200
maxi(Qw)i'.
Let
@X
be
t}
Lemma
A
n62
w-(Q
MA
26
Sexp
--
,
’
”
’
)
3200
maxi(Qw)%i
Riki;
7,,th
;k*(Q;)J-k\
26)
S4exp(
),
(29)
‘Qyﬁ’T
-
ofﬁ’ﬁ”,
(27)
Of
x
as
‘syii’T
7
mgﬁlﬁf
(28)
y=x+H’
z‘
(26)
By
combini
(31)
we
obt
1g
an
upper
bound
on
man-(Qwﬁi,
we
can
obtain
an
‘und
on
the
above
probability.
tining
the
above
bound
and
(32),
and
substituting
into
obtain
the
claimed
bound
in
the
lemma
El
3.
For
3X
deﬁned
in
(33),
we
have
2
have
LC
last
inequality
is
obtained
by
the
error
bound
of
2
for
the
sample
size
71/2.
I]
hat
the
matrix
EX
in
(33)
is
not
necessarily
positive
inite.
But
this
does
not
affect
the
convexity
of
the
tion
problem
(3).
)stituting
EX
from
(33)
into
the
(3),
we
can
solve
the
[I
ed
maximum
likelihood
problem
and
obtain
a
sparse
for
the
precision
matrix
@X.
Similar
to
Theorem
I,
guarantee
that
Uncoded
method
can
correctly
recover
rlying
graph
structure
with
high
probability.
1
2.
Consider
a
normal
distribution
Satisfying
the
Vice
Assumption
I
and
2
with
parameter
(I
E
(0,
l].
e
the
solution
of
the
log-determinant
program
(3)
with
nposing
ﬁ"ﬁ'T
as
UAZUT,
we
have
we
deﬁne
the
approximate
sample
covaliance
matrix
1
is
a
d
X
a'
zero
matrix.
y‘k
-
(Qx)jk|
Z
5)
“52)“
+
(Si)(d+j)(d+l<)
-
2(Qi)ik‘
Z
25)
‘(s£)jk
,
(mel
+
|(s£)(d+/)(d+k)
,
(Qxb'k‘
Z
25)
‘(S£)jk
,
(Q2)jk|
Z
5)
+
r
(‘(Si)(d+j)(d+l<)
,
(Qi)jk‘
Z
6)
4162
P
2L.
.
I
[[d
0d]
52
l0:
+
[0"
11115,:
W)
<33)
)ii
:
WE‘NQX
+
(TA-zﬁilﬁiTM
,
:
maxﬂQ‘I
+
ail-741714
l
)1!
:
m'ax(Qx),-,
+
mfxwfﬁ-WJM
(32)
,
62
m
‘(Qx)jl<|
2
6)
s
8exp(
:6
).
(
34)
l
5
{(32),}
+
(Si)(d+/)(d+l<))
«
section,
the
performance
of
our
proposed
methods
is
l
by
performing
several
experiments.
In
our
simula-
l8
glasso
package
[19]
is
used
to
solve
[1
-regularized
the
precision
matrix.
This
package
is
based
on
the
ordinate
descent
algorithm
proposed
by
[6].
'
experiments,
a
sparse
random
precision
matrix
is
i
as
follows.
First,
we
generate
a
random
sparse
graph
lxed
probability
of
the
edge
presence,
say
0.1,
and
ts
maximum
node
degree
to
A
=
5,
Then,
we
choose
ghts
uniformly
in
[-l,
l]
for
the
symmetric
precision
t
Next,
we
make
it
positive
deﬁnite
matrix
by
adding
identity
matrix.
Finally,
we
normalize
the
precision
3
set
the
variances
to
1,
Also,
we
ensure
that
the
1
matrix
satisﬁes
Assumption
1,
1ploy
the
True
Positive
and
False
Positive
Rates
(TPR
,
respectively)
as
our
performance
measures.
TPR
is
.s
the
percentage
of
the
predicted
edges
(non-zero
off-
entries
in
the
precision
matrix)
that
correctly
detected.
',
FPR
is
the
percentage
of
the
predicted
non-
edges
ries
in
the
precision
matrix)
that
incorrectly
detected.
we
experimentally
observed
that
Anugm
~
41;!0ngm‘”
OM
Azunginm;
where
Andgns,
A:"“C°ds".and
Anonginal
test
regsulanzation
parameter
for
the
signs,
uncoded,
nal
data,
respectively.
ler
to
have
a
fair
comparison
between
Signs
and
methods,
we
have
used
identical
parameters
for
the
from
the
ﬁg
large
enougl
In
Figure
different
val
a
random
g1
A
:
5
and
n
the
channel
normal
sam]
channel
ma
greater
than
close
to
the
The
FPR
negligible
e'
In
this
pa
of
GGMs
w
machines.
T
the
local
m2
Uncoded
m
shown
that
graph
if
la1
central
mac]
Our
expe
Signs
methc
Invariance
Ex
in
(33)
and
regularizatitm
parameter
channel.
MI
/a)1’zl-n
In-
for
some
0
<
6
<
d
".2
all
the
local
rate
of
sac}
xe
sample
size
is
lower
bounded
as
nrnnmed
ITII
2.
Since
the
channel
has
real
and
imaginary
parts,
2
can
be
transmitted
by
each
channel
access
(ie.
each
chine
can
transmit
n
samples
by
n/2
channel
uses),
2,
if
the
sample
generatian
rate
at
the
source
is
less
'qual
to
twice
of
the
channel
's
rate,
the
machines
can
all
samples
without
any
sample
loss,
cwr
In
Figure
for
a
star-s]
underlying
5
matrix
is
ge
(QX)[j
:
%
The
probab
urce
code
is
available
at
hups://gilhubvcom/ArminKuramzade/
haf’?
small
sparse-00M.
orlglnal
dat;
with
probability
at
least
l-dzé,
the
edge
set
speciﬁed
and
FPR
as
‘x
is
a
subset
of
the
true
edge
set.
10,000
In
I
3
sample
size
satisﬁes
the
lower
bound
20
different
.
3
2
Cuncoded
=
6V26
de(KzKr,KZK1-
.
be
seen,
by
Fr
(M(@)X;
a»)
2
1
7
die
(37)
:3;:;::::
Figure
function
-
ZV-cmax(K1-9m:m
3A
max{K;K1-,
nglz-H
[unvluvovu
8
2
8
n/2
channel
2
)ded
A
(1+
7)
1“
’v
(35)
bit-rate
of}
(l
1:“
l’
7’)
(36)
FPR
are
av:
be
seen
fro
method.
Ho
‘ERIMENTS
:ach
local
machine,
i.e.
R,,
to
2
bits.
Thus,
in
the
methods,
each
local
machine
can
transmit
11
bits
by
me]
uses.
According
to
(12),
in
order
to
achieve
the
)f
2,
the
signal
to
noise
ratio
(SNR)
should
set
to
3
:
3).
ﬁrst
experiment,
we
evaluate
the
performance
of
our
with
respect
to
the
dimension
(1.
Figure
2
shows
TPR
as
a
function
of
d
for
sample
sizes
n
:
1000
and
[n
this
experiment,
the
error
curves
are
averaged
over
ent
random
graphs
and
for
each
graph
the
TPR
and
averaged
over
10
different
random
samples.
As
can
from
Figure
2,
Signs
method
outperforms
Uncoded
However.
all
three
methods
have
approximately
the
R.
3
reﬂects
the
performance
of
the
methods
as
a
of
the
sample
size
n
for
d
:
50
and
(I
:
100.
As
can
by
increasing
the
sample
size
n,
the
performance
of
)ds
increases.
In
this
experiment
again
Signs
method
‘ms
the
uncoded
scheme
ure
4,
the
probability
of
perfect
structure
recovery
r-shaped
graph
is
depicted.
In
this
experiment,
the
1g
star
graph
consists
of
d
:
70
nodes.
The
precision
generated
as
the
inverse
of
a
covariance
matrix
with
%
for
all
(Lj)
e
8
which
satisﬁes
Assumption
1.
)
ability
of
perfect
recovery
is
estimated
by
running
150d
mcthods
100
times
and
counting
the
number
of
[I
the
structure
is
recovered
exactly.
As
can
be
seen
ﬁgure,
all
methods
recover
the
structure
exactly
for
)ugh
sample
sizes
as
claimed
by
theorems
l
and
2.
ure
5,
we
measure
the
TPR
of
Uncoded
method
for
values
of
the
SNR.
The
experiment
is
performed
on
1
graph
with
d
=
40
nodes
with
maximum
degree
of
d
n
:
10,
000.
In
this
experiment,
we
have
generated
nel
matrix
H
with
entries
drawn
from
i.i.d.
standard
amples.
The
TPR
curve
is
averaged
over
100
different
matrices.
As
can
be
seen
from
Figure
5,
for
SNR
ham
5,
the
performance
of
Uncoded
method
is
very
[he
TPR
of
the
original
data.
PR
curve
is
not
plotted,
since
the
error
values
were
e
even
for
small
SNR.
paper,
we
have
studied
the
sparse
structure
leaming
s
where
the
data
are
distributed
across
multiple
local
3,
Two
methods
are
proposed
to
send
information
from
machines
to
the
central
machine,
namely,
Signs
and
methods.
We
have
analytically
and
experimentally
tat
the
central
machine
can
recover
the
underlying
large
enough
sample
sizes
are
transmitted
to
the
iachine.
Xperimenls
show
that,
under
the
same
conditions,
:Ihnd
outperforms
the
uncnded
scheme.
Both
methods
all
FPR
which
is
close
to
[he
FPR
obtained
by
the
jam.
More
precisely,
we
assume
H
:
l
which
ensures
Ical
machines
have
identical
bit
rates,
We
set
the
bit
NCLUSION
l‘PR
and
FPR
as
a
function
of
the
sample
me
n
for
random
graph:
with
so
and
100
nodes,
935-980,
[5|
3.
Sojoud
”Mu”
«v
,
Probability
of
perfect
slruclure
recovery
for
the
star
graph
with
PPR
as
a
runcuon
orSNR
in
Uncoded
mm
for
a
random
graph
diménsiiu}
D
and
maximum
degree
A
=
5.
IlfMllt'hil
.
.
.
.
.
.
V
14.“;
ank
Amlr
Najaﬁ
and
Amlr-Hossem
Saber!
for
their
“3'
gl.)Chen,
comments
that
greatly
improved
the
manuscript.
mixed
gr;
‘me
‘SVIW
«Mum
-m
m
.9...
W
-nmw
W
‘
W
:~'~‘~3‘4
:33:
-WW
-WW
m
m
é
a
é
E
am
3
M
E
one
s
E
g
%
E
%
lwmmim
5mm
5m
0
wmmmm‘w
saw.
a.
u
mmwmwmim
my.
Em
smw
Sun
a
wwmwm‘m
on
1
non
+0ngwnal
Data
-S\gns
Melhud
+
Unwded
Memod
1500
2000
2500
soon
3500
won
Sample
Sue
[[
[2
[3
[4
-Ong‘\na|
Data
-
Unwded
Memod
5
10
15
20
25
SNR
(b)
d
:
50
,
:
d
,
(c)
100
(d)
d
[00
:KNOWLEDGMENT
Chai,
s.
K.
Loh.
s,
T,
Low.
M.
s.
Mohanrad,
s,
Del‘is.
and
2,
2a.
“
A
review
on
the
compumlional
approaches
for
gene
regularory
rk
conslruclion,"
Conipnierr
in
biology
(1/111
medicine.
vol.
48.
pp.
.,
2014.
eckei,
s.
Lambeck.
s.
Taeprer,
E.
Van
Sumeren,
and
R.
Gulhke,
2
regnlarory
network
inference:
data
inlegrarion
in
dynamic
mode
ireyiew."
81'
mm,
vol.
96.110.
I,
pp.
867103
2009.
ang,
1.
Nevllle.
and
M.
Rogaii,
“Modeling
relullollship
slrenglh
linc
soclul
llclwul‘ks
in
Pmt'eedillgs
ofihe
19in
miernrnnninl
rem-c
on
World
wide
n
b.
ACM.
2010.
pp.
9817990,
\likumzlr,
M,
1.
Wainwrighr,
G.
Rnskulti,
B.
in.
a:
111.,
“
High-
onal
covariance
eslimalion
by
minimizing
ippenalized
logr
ninani
divergence,"
Elecinnnc
Jullrlml
of
51111131163,
vol.
5,
pp.
180,
2011.
ioudi.
“
Equlvalence
of
graphical
laam
and
lhrexhnlding
rm
\parse
The
Journal
nfMuL/lillo
Lmrnrng
Rankin-1i,
vol.
17.
no.
I,
pp.
73963.
2016,
incrjcc.
L.
E.
Ghacur.
and
A.
d'Aspl'Cmom,
“Model
sclccuoll
gh
sparse
maximum
likelihood
enimalion
ror
multivariate
guusoiun
lary
dam,"
lonrnnl
of
Mar/line
Icul‘lll'llg
rcrcnrr-lr.
vol.
9.
no.
Mar,
157516,
2008.
low
and
C.
Lin.
“
Appl'oxlmalillg
discrele
pl'obublllly
dlslribullons
dcpcndcncc
lrees,"
15155
imnmcliona
on
Ill/Urllmlmn
Them-y.
4.
no.
3.
pp.
462-467,
1968,
nandkurnan
v.
v,
Tan.
F,
Huang.
and
A,
s,
Willoky,
“High-
lsional
gallssian
graphical
model
selection:
Walk
slllnmabilily
and
paranon
crirerion."
Journal
of
Machine
Lenrllillg
Rt'xemrll,
and
M.
T.-M.
Shahnani.
“
Learning
slmclured
gan
ian
graphical
models
on
disrrihnred
dara
under
lunicalion
consirainiss'
IEEE
Tn1ll.ulL/i1)ll.1
nn
Slglltll
Pnn-earing,
Tan.
A.
Anandkumar,
and
A.
s.
Wlllnky,
“Lc‘ru'nlng
hlghr
lsional
markon
flares!
distributions:
Analysis
ofermr
rules."l1lurmll
('th‘
mining
Remain-1r.
\lol.
12.
no.
May.
pp.
I6l7rl653,
201
1.
Tan.
A.
Anandknmar.
L.
Tong,
and
A.
s.
Willsky.
‘
A
large
ian
unulya
of
ihe
muxilllum-Ilkelihuod
learning
of
markov
iree
ures,"
IF
E
Tmninriinnr
on
Information
Theory.
vol,
57,
no.
3,
11101735,
2011.
einshausen,
1>.
Brihlmann
an
111.,
“
High-dimensional
graphs
and
ilc
sclccllon
wrrh
lhc
lasso."
The
annals
of
.rmrirnca.
vol,
34.
no.
3,
13671462,
2006.
en,
D.
M.
willen,
and
A.
Shujaie.
“
Selection
and
ealimaiion
1m
1
graphical
modelx,"
Bnnnanikn,
vol.
102.
no.
1,
pp,
47-64,
2014,
VCES
Armin
Karamzade
is
M.
Sc.
smdem
of
Artiﬁcial
Intelligence
at
the
Sharif
University
of
Technol»
ogyi
Tehran,
Iran.
He
received
his
B.
Sc.
degree
in
Compmer
Engineering
from
Iran
Unive
'iy
of
Science
and
Technology,
Tehran,
Inn.
“
1
2017‘
His
rese
ch
inleresls
in'
ude
mostly
machine
learning,
optimizmion,
and
mu
ic.
Hsren.
M.
A.
Sustik,
1.
s.
Dhillon,
and
P,
Ravrkumar,
“Quic:
atic
approximation
for
sparse
inverse
covariance
estimation,“
The
at
of
Machine
Learning
Research.
vol.
15.
n0.
1.
pp.
291172947,
Rothman,
P.
1.
Brckel,
E.
Levina,
J.
Zhu
et
at,
“Sparse
permutation
ant
covariance
estimation."
Electmnit:
Jtmmal
omelitlict.
vol.
2,
)LSIS,
2008.
eng.
D.
Wei.
A.
Wiesel.
and
A.
Hero
III.
“
Distributed
learning
ussiztn
graphical
models
Via
marginal
likelihoods,"
in
Arliﬁcizrl
ence
and
Slaiixzicx,
2013,
pp.
3947.
iiesel
and
A.
0.
Hero.
“
Distributed
covariance
estimation
in
ian
graphical
models,"
IEEE
Tranmctiunx
an
Signal
Pracexxirlg,
to,
no.
1,
pp.
211-220,
2012.
myo
and
E.
Hou,
“Efﬁcient
distributed
estimation
of
inverse
iance
matrices."
in
Statistical
Signal
Processing
Workshop
(SSF),
IEEE.
IEEE.
2016.
pp,
1-5.
edrnan,
T.
Hastie,
and
R.
Tibshirani,
“Sparse
inverse
covariance
ation
with
the
graphical
lasso."
Biosmlislics,
vol.
9,
no.
3.
pp.
4327
2008.
Se
and
P,
Viswanalh,
Fzmdumenmlx
(If
WilEIEJ‘S
cammzmit‘atian.
rridge
university
press,
2005.
Bacon.
‘
Approximations
to
multivariate
nonnal
onhant
pmbubillr
The
Armalr
omehemarical
Statistics,
vol.
34,
no.
l,pp.
191-198.
l
Gama]
and
L.
Lal.
"
On
rate
requirements
for
achieving
the
llized
perfumtance
in
di
l‘ibuted
estimation."
IEEE
Trunmcliun:
elm!
Meaning.
vol.
65.
no.
8.
pp.
2020-2032,
2017.
July
2013.
he
Engineering
an
research
mtel‘es
received
severa
Council
of
Car
Mostafa
Tavassolipour
received
the
B.Sc.
degree
from
Shahed
Universityv
Tehran
Iran,
in
2009»
and
the
M.Scr
degree
from
Computer
Engineering
de-
parlment
of
Sharif
University
of
Technology
(SUT),
Tehran.
Iran.
in
201].
Currently.
He
is
a
PhD.
stu»
dent
of
Artiﬁcial
Intelligence
program
at
Computer
Engineering
Department
of
Sharif
University
of
Technology.
His
research
interests
include
machine
learning.
image
processing.
inform
tion
theory,
con»
tent
based
video
analysisv
and
bioinformatics,
Reza
Mirzaeil'ard
received
the
B.Sc.
degree
of
come
puter
engineering
from
Shahid
Beheshti
University,
Tehran,
Iran,
in
2017.
Currently,
He
is
M.Scr
stu-
dent
of
Artiﬁcial
Inielligence
program
at
Computer
Engineering
Department
of
Sharif
University
of
Technology,
His
research
interests
include
machine
learning,
information
theory,
tensor
decomposition,
bioinformalics.
Seyed
Abolf‘azl
Motahari
i:
an
assistant
profes-
sor
at
Compiirer
Engineering
Department
of
Sharif
University
of
Technology
(SUT).
He
received
his
1350,
degree
from
the
lran
University
of
Science
and
Technology
(IUST),
Tehran,
in
I999,
the
M.SC.
de-
gree
from
Sharif
University
of
Technology
Tehrani
in
2001.
and
the
PhD.
degree
from
University
ct
Waterloo
Waterloo»
Canada.
in
2009‘
all
in
electrical
engineering.
From
October
2009
to
September
2010,
he
was
in
Postdoctoral
Fellow
with
the
University
of
Waterloo,
Waterloo.
From
September
2010
to
he
was
a
Poudoctoral
Fellow
with
the
Department
of
Electrical
g
and
Computer
Sciences»
University
of
California
at
Berkeley,
Hit
leresls
include
muliiuser
information
theory
and
Bioinforinulics.
He
Vera]
awards
including
Natural
Science
and
Engineering
Research
Canada
(NSERC)
Post-Doctoral
Fellowship.
Mohammad-Taghi
Manzuri
Shalmani
received
the
BVSC.
and
MVSeV
m
clcclricu]
engineering
from
Sharif
University
of
Technology
(sun
Iran,
in
1984
and
1988,
respectively
He
received
the
PhD,
degree
in
elecn-ienl
and
compuler
engineering
from
the
V|Cnml
University
of
chhnology,
Ausiria,
in
1995,
Currently.
he
is
an
associate
professor
in
the
Cornpurer
Engineering
Depnrlmenl
Sharif
Univep
shy
of
Technology,
Tehran
Iran.
His
main
research
micresis
nclude
dig’
lsignulpmcessmg
slochustic
modeling,
and
Mulli-
resolulion
signal
proces
ng