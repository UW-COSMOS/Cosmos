universe = docker

# request_disk should be at least 10x the size of the input files
# processing propose typically uses about 3.5 GB of CPU memory and 8GB of GPU memory
# add cpus=xx disk=yy memory=zz on the submit command line to override these defaults
request_disk = $(disk:500MB)
request_memory = $(memory:6GB)
request_cpus = $(cpus:1)
request_gpus = 1
require_gpus = GlobalMemoryMb >= 8192

# choose a docker image, by default we will use the newer one
# the image then implies a GPU capability requirement
if $(older_docker_image)
  docker_image = uwcosmos/cosmos-ingestion
  require_gpus = $(require_gpus) && capability >= 3.7 && capability <= 7.5
else
  docker_image = uwcosmos/cosmos-ingestion-c11
  require_gpus = $(require_gpus) && capability > 3.7
endif
require_gpus = $(require_gpus) && GlobalMemoryMb >= 8192

# if the submit was invoked with use_cpu=1, or propose=1 on the command line
# do a cpu-only run by clearing the GPU request
if $(propose)
    use_cpu = 1
endif
if $(use_cpu)
    request_gpus =
    require_gpus =
    request_cpus = 1
endif

# tell CHTC pool that we want to match backfill resources
+WantFlocking=true
Requirements = versionGE(Microarch,"x86_64-v3")

# pdf files in INPUT_DIR, parquet files in OUT_DIR, temp pages and pickles in INFO_DIR
# contents of the INFO_DIR will be treated as both input and output
# INPUT_DIR can be set to the full path to the input  PDF files (use from=<path> on the command line)
# but this submit file expects OUT_DIR and INFO_DIR to be relative to CWD at submit time
INPUT_DIR = $(FROM:input)
OUT_DIR = output
INFO_DIR = pages

# This controls what temporary data we return in the INFO_DIR
# This only filters *new* files. it has no effect on files already in the INFO_DIR
KEEP_INFO=pickle,page,pad

# paths to the model in the docker image. these will get put into the job environment
MODEL_CONFIG=/configs/model_config.yaml
WEIGHTS_PTH=/weights/model_weights.pth

# pass configuration info to the job in the environment
env = ;MODEL_CONFIG=$(MODEL_CONFIG);WEIGHTS_PTH=$(WEIGHTS_PTH);KEEP_INFO=$(KEEP_INFO);SKIP_AGGREGATION=True

# the actual job, the script will get copied from the submit machine
# but we will use the python that is installed in the docker image
SCRIPT = make_parquet.py
executable = /usr/bin/python3.8
transfer_executable = false
arguments = $Fnx(SCRIPT) $Fn(INPUT_DIR) $(INFO_DIR) $(OUT_DIR)

transfer_input_files = $(SCRIPT), $(INPUT_DIR), $(INFO_DIR)
transfer_output_files = $(OUT_DIR), $(INFO_DIR)
should_transfer_files = YES
when_to_transfer_output = ON_EXIT_OR_EVICT

if $(append)
   erase_output_and_error_on_restart = false
endif
output = $(CLUSTERID).out
error = $(CLUSTERID).err
log = infer.log

queue
